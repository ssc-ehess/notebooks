{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ba7d2e-8a55-49ad-bbd2-cbf14c89f3c8",
   "metadata": {},
   "source": [
    "# Bases du traitement automatique des textes\n",
    "\n",
    "Ce notebook couvre:\n",
    "\n",
    " - Prétraitement des textes: tokenization (transformation des textes en séquences de phrases, mots, ou autre unités linguistiques); lemmatization (réduction à la racine); etc.\n",
    " - Construction de réseaux de co-occurrence.\n",
    " - Comparaison de fréquences d'occurrence.\n",
    "\n",
    "On commence par importer les packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "1593eef8-ed92-4104-b1ad-74ff54998e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f67a3-4660-4367-932a-13afa80ccd79",
   "metadata": {},
   "source": [
    "(Si l'importation de NLTK échoue, fermez jupyter, tapez `pip install nltk` dans votre terminal, puis relancez jupyter) \n",
    "\n",
    "La deuxième étape est le téléchargement de quelques fonctionnalités nltk de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "28b9167f-58f6-40bd-ae0e-2cc4fa07688f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lucasgautheron/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lucasgautheron/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/lucasgautheron/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets_json to\n",
      "[nltk_data]     /Users/lucasgautheron/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('tagsets_json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc857f25-79c8-4e83-9e55-3d7e48ce05dc",
   "metadata": {},
   "source": [
    "## Pré-traitement des textes: tokenization, lemmatization, et filtres lexicaux\n",
    "\n",
    "Nous allons d'abord aborder la tokenzation, une étape cruciale dans le traitement automatique des textes.\n",
    "Cela peut couvrir, par exemple, la transformation d'un document en une séquence de mots. \n",
    "NLTK permet de réaliser cette opération:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "0852375e-5354-4b2d-b869-a31d5b3e9b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'apples', '.', 'And', 'also', 'strawberries', '.']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.tokenize.word_tokenize(\"I love apples. And also strawberries.\")\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccf59d-03d8-42d0-aa22-d33beeaf6166",
   "metadata": {},
   "source": [
    "Le processus fournit une liste de mots (en plus des éléments de ponctuation).\n",
    "\n",
    "Il est fréquemment utile de réduire chacun de ces mots à leur racine, qui contient presque toute l'information sémantique utile. Par exemple, en pratique, il n'y a pas de distinction sémantique entre \"strawberry\" et \"strawberries\".\n",
    "\n",
    "Pour cela, on a recours à la lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "0d2dae91-5654-439a-a5d7-5c5f8380084c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'apple', '.', 'And', 'also', 'strawberry', '.']"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stems = [\n",
    "    lemmatizer.lemmatize(word) for word in words\n",
    "]\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f01039-896b-4db5-84cd-fdc34e3ac229",
   "metadata": {},
   "source": [
    "Dans certains cas, les mots tels que \"I\" ou \"And\" sont une distraction (ils ne portent aucune information sémantique utile).\n",
    "Une manière de s'en débarrasser est de filtrer les mots selon leur catégorie lexicale. Celle-ci peut-être déterminée via la fonction \"part-of-speech tagging\" (pos tagging) de NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "6d0928bc-cf1b-4761-af50-0db644c246e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('apples', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('also', 'RB'),\n",
       " ('strawberries', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631e426-1a66-466b-9626-2e596308849c",
   "metadata": {},
   "source": [
    "Pour comprendre la signification des tags, on peut consulter le registre suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "5c49f87d-89a0-4329-b045-9d4c2081b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980137e-4563-4db3-aa14-9af76448ea5f",
   "metadata": {},
   "source": [
    "Ainsi, pour filtrer les noms et les adjectifs:\n",
    " - on itère sur chaque paire (mot, tag) [par example: ('apple', 'NNS')]\n",
    " - on retient tous les éléments de tag commençant par NN (noms) ou JJ (adjectifs)\n",
    "\n",
    "Voici comment le faire avec une \"compréhension de liste\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "44d97cd3-a375-4e8a-b0d6-02949e9d1eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apple', 'strawberry']"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\n",
    "    word\n",
    "    for word, tag in nltk.tag.pos_tag(words)\n",
    "    if tag[:2] == \"NN\" or tag[:2] == \"JJ\"\n",
    "]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108de1b-5f58-4823-b750-e2c3a9c4249b",
   "metadata": {},
   "source": [
    "On peut rassembler les étapes suivantes en définissant une fonction qui prend un texte en entrée, le découpe en mots (tokenization), filtre les noms et adjectifs, et les réduits à leur racine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "fd69d72d-97b1-47d8-8f2c-27095c2b1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(text):\n",
    "    text = text.lower() # on passe le texte en minuscules (important! sinon, \"strawberry\" et \"Strawberry\" sont considérés comme des mots distincts)\n",
    "    \n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    \n",
    "    tags = nltk.tag.pos_tag(words)\n",
    "    \n",
    "    words = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word, tag in tags\n",
    "        if tag[:2] == \"NN\" or tag[:2] == \"JJ\"\n",
    "    ]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "a1be5a92-5dab-43b2-b65a-1f0c88c58766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'green', 'apple', 'red', 'strawberry']"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_words(\"I love green apples. And also red strawberries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4abc5-6756-46f1-8bcb-03694028fcd4",
   "metadata": {},
   "source": [
    "On peut désormais appliquer cette fonction à notre corpus. Pour des raisons d'efficacité, nous allons l'appliquer aux titre des articles en sciences du climat. Mais on pourrait aussi l'appliquer aux résumés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "19f63cf4-f2e4-4b76-9113-41cf847677f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "4393546166             [data, et, al, uncertainty, job, seeker]\n",
       "4393714386             [data, et, al, uncertainty, job, seeker]\n",
       "4220805390    [urban, risk, europe, available, continental-s...\n",
       "4220871093    [carbon, footprint, assessment, decarbonisatio...\n",
       "4224019370        [future, cryosphere, impact, global, warming]\n",
       "                                    ...                        \n",
       "4386855500    [accounting, time, greenhouse, gas, emission, ...\n",
       "4389633312    [green, sukuk, saudi, arabia, challenge, poten...\n",
       "4389862110                   [esg, green, management, thailand]\n",
       "4389881210    [credibility, green, bond, market, gbers, case...\n",
       "4390419499    [development, green, finance, eaeu, country, a...\n",
       "Name: words, Length: 230400, dtype: object"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"science/climate/articles.parquet\")\n",
    "df.dropna(subset=[\"title\"], inplace=True) # retrait des articles de titre inconnu/vide\n",
    "df[\"words\"] = df[\"title\"].map(text_to_words) # création d'une nouvelle colonne \"words\" à partir du titre et de notre fonction text_to_words\n",
    "df[\"words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "00275bae-f95d-444d-a4b5-c97ecc8dc953",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_frequency = dict()\n",
    "\n",
    "for words in df[\"words\"].tolist():\n",
    "    for word in set(words):\n",
    "        document_frequency[word] = document_frequency.get(word, 0)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "7dd8a054-f5ba-4706-b2b1-2c08f192dc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "366"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency[\"fossil\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "90858207-619f-48db-8b79-fa1da5fd8844",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_words = [\n",
    "    word for word in document_frequency.keys() if document_frequency[word] >= 100\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "3a65ddb0-8242-42da-82c1-ed85167ee022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['al',\n",
       " 'job',\n",
       " 'uncertainty',\n",
       " 'data',\n",
       " 'et',\n",
       " 'available',\n",
       " 'urban',\n",
       " 'europe',\n",
       " 'risk',\n",
       " 'development']"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "a89ff421-840e-4b78-bf8a-9f4528a1ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for words in df[\"words\"]:\n",
    "    words = [word for word in words if word in frequent_words]\n",
    "    words = list(set(words))\n",
    "    for i, a in enumerate(words):\n",
    "        for j, b in enumerate(words):\n",
    "            if i<j:\n",
    "                if G.has_edge(a, b):\n",
    "                    G[a][b][\"coocc\"] += 1\n",
    "                else:\n",
    "                    G.add_edge(a, b, coocc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "d87f7896-9a7b-4d7c-96a4-7cb39c8fb8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"output/coocc.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "514afbe0-8da5-40fe-b241-34a81b73b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "words = list(G.nodes)\n",
    "N = len(df)\n",
    "occ = {\n",
    "    word: document_frequency[word]/N for word in words\n",
    "}\n",
    "\n",
    "weight = dict()\n",
    "for a, b, attrs in G.edges(data=True):\n",
    "    weight[(a,b)] = np.log(occ[a]*occ[b])/np.log(attrs[\"coocc\"]/N) - 1\n",
    "\n",
    "nx.set_edge_attributes(G, weight, \"weight\")\n",
    "G.remove_edges_from([(a,b) for a, b, attrs in G.edges(data=True) if attrs[\"weight\"]<0.1 or attrs[\"coocc\"]<15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "74a2c5b6-cb62-4bc3-b34d-0e5aaa4ea79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"output/coocc.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "c84b9022-9bc2-4cfc-842a-b0a1a7fc904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_parquet(\"science/climate/authors.parquet\")\n",
    "articles_authors = pd.read_parquet(\"science/climate/articles_authors.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "41813960-1be9-4965-b1ce-1e91d59e4480",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_authors = articles_authors.merge(authors, left_on=\"author_id\", right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "1c33be4a-2bcf-4366-8419-ae780c684c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_gender = articles_authors.groupby(\"article_id\").agg(\n",
    "    male = (\"gender\", lambda g: np.sum(g==\"m\")),\n",
    "    female = (\"gender\", lambda g: np.sum(g==\"f\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "fb859005-6a21-4b31-96c3-61b7f77285c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(article_gender, how=\"inner\", left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "ad37e7f5-01a7-486d-b7bb-abd4793a8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(df):\n",
    "    document_frequency = dict()\n",
    "\n",
    "    for words in df[\"words\"].tolist():\n",
    "        for word in set(words):\n",
    "            document_frequency[word] = document_frequency.get(word, 0)+1\n",
    "\n",
    "    return document_frequency\n",
    "\n",
    "male_docs = df[df[\"male\"]>df[\"female\"]]\n",
    "female_docs = df[df[\"male\"]<df[\"female\"]]\n",
    "\n",
    "male_freq = term_frequency(male_docs)\n",
    "female_freq = term_frequency(female_docs)\n",
    "\n",
    "n_male = len(male_docs)\n",
    "n_female = len(female_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "14d9243d-c5b8-4412-8443-fadb42cb0783",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = []\n",
    "for word in frequent_words:\n",
    "    freq.append({\n",
    "        \"word\": word,\n",
    "        \"male\": male_freq.get(word, 0),\n",
    "        \"female\": female_freq.get(word, 0),\n",
    "        \"male_proportion\": male_freq.get(word, 0)/n_male,\n",
    "        \"female_proportion\": female_freq.get(word, 0)/n_female\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "3aa3efd8-2429-40f9-b036-0de3866d36ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>male_proportion</th>\n",
       "      <th>female_proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>398</td>\n",
       "      <td>123</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.002256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>88</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.000587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uncertainty</td>\n",
       "      <td>1493</td>\n",
       "      <td>407</td>\n",
       "      <td>0.012225</td>\n",
       "      <td>0.007466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>6464</td>\n",
       "      <td>2120</td>\n",
       "      <td>0.052928</td>\n",
       "      <td>0.038888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>et</td>\n",
       "      <td>305</td>\n",
       "      <td>107</td>\n",
       "      <td>0.002497</td>\n",
       "      <td>0.001963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2176</th>\n",
       "      <td>birdlife</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>v0.1</td>\n",
       "      <td>142</td>\n",
       "      <td>45</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>v0.2</td>\n",
       "      <td>71</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>cientific</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>pati</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2181 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  male  female  male_proportion  female_proportion\n",
       "0              al   398     123         0.003259           0.002256\n",
       "1             job    88      32         0.000721           0.000587\n",
       "2     uncertainty  1493     407         0.012225           0.007466\n",
       "3            data  6464    2120         0.052928           0.038888\n",
       "4              et   305     107         0.002497           0.001963\n",
       "...           ...   ...     ...              ...                ...\n",
       "2176     birdlife     0       0         0.000000           0.000000\n",
       "2177         v0.1   142      45         0.001163           0.000825\n",
       "2178         v0.2    71      22         0.000581           0.000404\n",
       "2179    cientific   312       0         0.002555           0.000000\n",
       "2180         pati   312       0         0.002555           0.000000\n",
       "\n",
       "[2181 rows x 5 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = pd.DataFrame(freq)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "f322e334-1499-466a-8651-608ca9283b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq[\"ratio\"] = freq[\"female_proportion\"]/freq[\"male_proportion\"]\n",
    "freq = freq[freq[\"male\"]+freq[\"female\"]>20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "09e45359-80df-4046-9860-dc3e97f32e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>male_proportion</th>\n",
       "      <th>female_proportion</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2180</th>\n",
       "      <td>pati</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2179</th>\n",
       "      <td>cientific</td>\n",
       "      <td>312</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>elite</td>\n",
       "      <td>111</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.121094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>lidar</td>\n",
       "      <td>180</td>\n",
       "      <td>14</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.174241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>plume</td>\n",
       "      <td>118</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.189851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1909</th>\n",
       "      <td>cop27</td>\n",
       "      <td>93</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.192709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>constant</td>\n",
       "      <td>129</td>\n",
       "      <td>12</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.208395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1703</th>\n",
       "      <td>tropomi</td>\n",
       "      <td>102</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.219632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>saudi</td>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.221806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>turbulence</td>\n",
       "      <td>90</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.224024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  male  female  male_proportion  female_proportion     ratio\n",
       "2180        pati   312       0         0.002555           0.000000  0.000000\n",
       "2179   cientific   312       0         0.002555           0.000000  0.000000\n",
       "2064       elite   111       6         0.000909           0.000110  0.121094\n",
       "2094       lidar   180      14         0.001474           0.000257  0.174241\n",
       "1787       plume   118      10         0.000966           0.000183  0.189851\n",
       "1909       cop27    93       8         0.000761           0.000147  0.192709\n",
       "1562    constant   129      12         0.001056           0.000220  0.208395\n",
       "1703     tropomi   102      10         0.000835           0.000183  0.219632\n",
       "950        saudi   101      10         0.000827           0.000183  0.221806\n",
       "1726  turbulence    90       9         0.000737           0.000165  0.224024"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# male dominated\n",
    "\n",
    "freq.sort_values(\"ratio\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "a32a9b67-d3ee-43a1-b467-0345498085e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>male</th>\n",
       "      <th>female</th>\n",
       "      <th>male_proportion</th>\n",
       "      <th>female_proportion</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2174</th>\n",
       "      <td>weecology/portaldata</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2175</th>\n",
       "      <td>weecology/forecasts</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>nursing</td>\n",
       "      <td>31</td>\n",
       "      <td>116</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>8.382839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>nurse</td>\n",
       "      <td>32</td>\n",
       "      <td>104</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.001908</td>\n",
       "      <td>7.280785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>curriculum</td>\n",
       "      <td>20</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>6.832736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>maternal</td>\n",
       "      <td>24</td>\n",
       "      <td>72</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>6.720724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>libguides</td>\n",
       "      <td>29</td>\n",
       "      <td>78</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>6.025477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>woman</td>\n",
       "      <td>134</td>\n",
       "      <td>355</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>5.934968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>pm2.5</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>5.914237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>emotional</td>\n",
       "      <td>26</td>\n",
       "      <td>61</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>5.255951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      word  male  female  male_proportion  female_proportion  \\\n",
       "2174  weecology/portaldata     0     242         0.000000           0.004439   \n",
       "2175   weecology/forecasts     0     172         0.000000           0.003155   \n",
       "1931               nursing    31     116         0.000254           0.002128   \n",
       "1954                 nurse    32     104         0.000262           0.001908   \n",
       "1979            curriculum    20      61         0.000164           0.001119   \n",
       "1923              maternal    24      72         0.000197           0.001321   \n",
       "84               libguides    29      78         0.000237           0.001431   \n",
       "261                  woman   134     355         0.001097           0.006512   \n",
       "1939                 pm2.5    25      66         0.000205           0.001211   \n",
       "1975             emotional    26      61         0.000213           0.001119   \n",
       "\n",
       "         ratio  \n",
       "2174       inf  \n",
       "2175       inf  \n",
       "1931  8.382839  \n",
       "1954  7.280785  \n",
       "1979  6.832736  \n",
       "1923  6.720724  \n",
       "84    6.025477  \n",
       "261   5.934968  \n",
       "1939  5.914237  \n",
       "1975  5.255951  "
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# female dominated\n",
    "\n",
    "freq.sort_values(\"ratio\", ascending=False).head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
